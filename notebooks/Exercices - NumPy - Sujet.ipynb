{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b573bca5",
   "metadata": {},
   "source": [
    "# Un peu d'apprentissage automatique avec NumPy\n",
    "\n",
    "Nous allons effectuer un petit peu d'apprentissage automatique, qui est une sous-branche de l'intelligence artificielle, pour illustrer l'utilisation du paquet `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2262cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38edfbfd",
   "metadata": {},
   "source": [
    "## Régression linéaire simple\n",
    "\n",
    "Supposons que l'on a une variable d'intérêt $y \\in \\mathbb{R}$, qui est continue, que l'on cherche à prédire à partir d'une variable en entrée continue $x \\in \\mathbb{R}$.\n",
    "On dispose de $n$ observations, qui sont représentées par le vecteur $\\mathbf{x} \\in \\mathbb{R}^n$ contenant les entrées et le vecteur $\\mathbf{y} \\in \\mathbb{R}^n$ contenant les sorties.\n",
    "On utilise la fonction [`sklearn.datasets.make_regression()`](https://scikit-learn.org/1.6/modules/generated/sklearn.datasets.make_regression.html) pour générer un tel jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14113b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "x_lin_sim, y_lin_sim = make_regression(\n",
    "    n_samples=40, n_features=1, n_informative=1, bias=2.0, noise=40, random_state=42\n",
    ")\n",
    "\n",
    "x_lin_sim = x_lin_sim.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b0a00",
   "metadata": {},
   "source": [
    "Deux variables Python ont été créées :\n",
    "\n",
    "* `x_lin_sim` correspond au vecteur $\\mathbf{x}$ contenant les entrées,\n",
    "* `y_lin_sim` correspond au vecteur $\\mathbf{y}$ contenant les sorties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077ca13",
   "metadata": {},
   "source": [
    "**Question 1 : Affichez les données avec un graphique adapté.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b6933",
   "metadata": {},
   "source": [
    "La régression linéaire simple fait l'hypothèse que la relation entre la variable d'intérêt et la variable en entrée est modélisée par une relation linéaire :\n",
    "\n",
    "$$\n",
    "    y \\approx a x + b\n",
    "$$\n",
    "\n",
    "Pour estimer le coefficient $a$ et la constante $b$, on utilise le critère de la somme des moindres carrés :\n",
    "\n",
    "$$\n",
    "    \\hat{a}, \\hat{b} = \\arg\\min_{a, b} \\sum_{i=1}^n \\left( y^{(i)} - (a x^{(i)} + b) \\right)^2\n",
    "$$\n",
    "\n",
    "Les solutions à ce problème d'optimisation (obtenues en cherchant quand le gradient de cette fonction est nul) sont les suivantes :\n",
    "\n",
    "$$\n",
    "    \\hat{a} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)} = \\frac{\\sum_{i=1}^n \\left( x^{(i)} - \\bar{x} \\right) \\left( y^{(i)} - \\bar{y} \\right)}{\\sum_{i=1}^n \\left( x^{(i)} - \\bar{x} \\right)^2} \\qquad\\text{et}\\qquad \\hat{b} = \\bar{y} - \\hat{a} \\bar{x}\n",
    "$$\n",
    "\n",
    "avec :\n",
    "\n",
    "$$\n",
    "    \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x^{(i)} \\qquad\\text{et}\\qquad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a5618",
   "metadata": {},
   "source": [
    "**Question 2 : Calculez les solutions de la régression linéaire simple pour ce jeu de données avec les formules fournies ci-dessus.** Vous pouvez utiliser la fonction [`numpy.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) qui renvoie la **matrice de covariance** entre deux vecteurs, ou implémenter la formule fournie ci-dessus. Les solutions approximatives sont :\n",
    "\n",
    "$$\n",
    "    \\hat{a} \\approx 86.91856 \\qquad\\text{et}\\qquad \\hat{b} \\approx 1.02764\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697f3e3",
   "metadata": {},
   "source": [
    "**Question 3 : Affichez la régression linéaire simple optimale avec le jeu de données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ee669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbf0f1",
   "metadata": {},
   "source": [
    "## Régression linéaire multiple\n",
    "\n",
    "Passons maintenant à un cas plus complexe, mais plus réaliste, avec la régression linéaire multiple.\n",
    "En effet, il est irréaliste dans l'immense majorité des cas d'espérer pouvoir prédire la variable d'intérêt $y$ avec une seule variable en entrée.\n",
    "\n",
    "On suppose maintenant que chaque entrée n'est plus un réel $x^{(i)} \\in \\mathbb{R}$ mais un vecteur de nombre réels $\\mathbf{x}^{(i)} \\in \\mathbb{R}^p$.\n",
    "À chaque entrée $\\mathbf{x}_i$ est toujours associée une sortie $y^{(i)} \\in \\mathbb{R}$.\n",
    "Les entrées sont représentées par la matrice $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ et les sorties sont représentées par le vecteur $\\mathbf{y} \\in \\mathbb{R}^{n}$.\n",
    "\n",
    "On utilise à nouveau la fonction [`sklearn.datasets.make_regression()`](https://scikit-learn.org/1.6/modules/generated/sklearn.datasets.make_regression.html) pour générer un tel jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin, y_lin = make_regression(\n",
    "    n_samples=40, n_features=10, n_informative=8, bias=70.0, noise=40, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b27fc",
   "metadata": {},
   "source": [
    "Deux variables Python ont été créées :\n",
    "\n",
    "* `X_lin` correspond à la matrice $\\mathbf{X}$ contenant les entrées,\n",
    "* `y_lin` correspond au vecteur $\\mathbf{y}$ contenant les sorties.\n",
    "\n",
    "La régression linéaire multiple fait l'hypothèse que la relation entre la variable d'intérêt et les variables en entrée est modélisée par une relation linéaire :\n",
    "\n",
    "$$\n",
    "    y \\approx w_0 + \\sum_{j=1}^p w_i x_i\n",
    "$$\n",
    "\n",
    "Une manière courante de rajouter la constante est de rajouter une variable supplémentaire qui vaut 1, c'est-à-dire une colonne de 1 à la matrice $\\mathbf{X}$ :\n",
    "\n",
    "$$\n",
    "    y \\approx \\sum_{j=0}^p w_i x_i = \\mathbf{w}^\\top \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b66d79",
   "metadata": {},
   "source": [
    "**Question 4 : Ajoutez une colonne de 1 en tant que dernière colonne à la matrice `X_lin` pour modéliser la constante.** Vous pouvez utiliser les fonctions [`numpy.ones()`](https://numpy.org/doc/stable/reference/generated/numpy.ones.html) pour créer un tableau ne contenant que des 1 et [`numpy.column_stack()`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html), [`numpy.hstack()`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) ou [`numpy.concatenate()`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) pour concaténer deux tableaux NumPy le long des colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf27707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d27e2",
   "metadata": {},
   "source": [
    "On utilise toujours le critère de la somme des moindres carrés pour trouver les paramètres optimaux :\n",
    "\n",
    "$$\n",
    "    \\hat{\\mathbf{w}} = \\arg\\min_{\\mathbf{w}} \\sum_{i=1}^n \\left( y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}_i \\right)^2\n",
    "    = \\arg\\min_{\\mathbf{w}} \\Vert \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\Vert_2^2\n",
    "$$\n",
    "\n",
    "On calcule le gradient de cette fonction par rapport à $\\mathbf{w}$ et on cherche quand il est nul :\n",
    "\n",
    "$$\n",
    "-2 \\mathbf{X}^\\top \\left( \\mathbf{y} - \\mathbf{X} \\hat{\\mathbf{w}} \\right) = 0\n",
    "\\Longleftrightarrow \\mathbf{X}^\\top \\left( \\mathbf{y} - \\mathbf{X} \\hat{\\mathbf{w}} \\right) = 0\n",
    "\\Longleftrightarrow \\mathbf{X}^\\top \\mathbf{X} \\hat{\\mathbf{w}} = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "On obtient donc l'équation suivante :\n",
    "\n",
    "$$\n",
    "    \\mathbf{X}^\\top \\mathbf{X} \\hat{\\mathbf{w}} = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Vous avez peut-être envie d'inverser la matrice $\\mathbf{X}^\\top \\mathbf{X}$ pour trouver $\\hat{\\mathbf{w}}$, mais ce n'est pas une bonne idée.\n",
    "En effet, on n'a pas besoin de $\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}$, qui est une matrice de taille $(p + 1) \\times (p + 1)$, mais seulement de $\\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$, qui est un vecteur de taille $p + 1$.\n",
    "C'est pourquoi on laisse l'équation sous la forme d'un système d'équations linéaires, parce qu'il est plus efficace de la résoudre sous cette forme :\n",
    "\n",
    "$$\n",
    "    \\mathbf{X}^\\top \\mathbf{X} \\hat{\\mathbf{w}} = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "**Question 5 : La fonction [`numpy.linalg.solve()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html) permet de résoudre un système d'équations linéaires. Utilisez cette fonction pour trouver les paramètres optimaux $\\hat{\\mathbf{w}}$ de la régression linéaire multiple des moindres carrés.** Vous pouvez utiliser la propriété [`.T`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html#numpy.ndarray.T) pour transposer un tableau NumPy à deux dimensions. On rappelle que le produit matriciel peut être obtenu avec l'opérateur `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff679c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e3b90",
   "metadata": {},
   "source": [
    "Maintenant que l'on a estimé les paramètres $\\hat{\\mathbf{w}}$, on peut obtenir la prédiction effectuée par le modèle pour n'importe quelle observation :\n",
    "* Pour une seule observation $\\mathbf{x}$ : $\\displaystyle \\hat{y} = \\hat{\\mathbf{w}}^\\top \\mathbf{x} = \\mathbf{x}^\\top \\hat{\\mathbf{w}}$\n",
    "* Pour un lot d'observations $\\mathbf{X}$ : $\\displaystyle \\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\mathbf{w}}$\n",
    "\n",
    "**Question 6 : Calculez les prédictions $\\hat{\\mathbf{y}}$ obtenues avec ce modèle pour toutes les observations $\\mathbf{X}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4efc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516bc2e6",
   "metadata": {},
   "source": [
    "Pour évaluer la qualité de notre modèle, nous allons utiliser comme critère la racine carrée de l'erreur quadratique moyenne (RMSE pour *root mean squared error* en anglais) :\n",
    "\n",
    "$$\n",
    "    \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2}\n",
    "$$\n",
    "\n",
    "**Question 7 : Calculez la racine carrée de l'erreur quadratique moyenne entre les vraies sorties et les sorties prédites.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911eaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50fecf",
   "metadata": {},
   "source": [
    "Quand on fait de l'apprentissage automatique supervisé, on souhaite évaluer le modèle sur un jeu de données indépendant du jeu d'entraînement.\n",
    "\n",
    "**Question 8 : Utilisez la classe [`sklearn.model_selection.ShuffleSplit()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html) pour séparer 10 fois le jeu de données complet en un jeu d'entraînement avec 80% des observations et un jeu d'évaluation avec 20% des observations, répondre à nouveau aux questions 5 à 7 et calculer la moyenne des racines carrées des erreurs quadratiques moyennes. Comparez le score obtenu ici avec celui obtenu à la question précédente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eaf9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# On sauvegarde les résultats dans une liste\n",
    "res = []\n",
    "\n",
    "# On crée l'instance de sklearn.model_selection.ShuffleSplit() avec les bons arguments\n",
    "shuffle_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=43)\n",
    "\n",
    "for train_idx, test_idx in shuffle_split.split(X_lin):\n",
    "    # train_idx est un tableau NumPy contenant les indices du jeu d'entraînement\n",
    "    # test_idx est un tableau NumPy contenant les indices du jeu d'évaluation\n",
    "\n",
    "    # TODO : Créer les tableaux pour les jeux d'entraînement et d'évaluation\n",
    "\n",
    "\n",
    "    # TODO : Calculer les paramètres w_hat sur le jeu d'entraînement\n",
    "\n",
    "\n",
    "    # TODO : Calculer la RMSE sur le jeu d'évaluation et la sauvegarder dans res\n",
    "\n",
    "\n",
    "# TODO : Calculer la RMSE moyenne sur les 10 jeux d'évaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1dcff",
   "metadata": {},
   "source": [
    "## Régression logistique\n",
    "\n",
    "La régression logistique est un modèle linéaire pour des tâches de classification binaire.\n",
    "L'objectif n'est pas de prédire un nombre réel mais une classe parmi deux choix possibles (par exemple *malade* ou *sain* dans le domaine médical).\n",
    "L'algorithme s'étend aux tâches de classification multiclasse, c'est-à-dire avec un nombre de classes strictement plus grand que deux, mais nous nous limiterons à la classification binaire ici.\n",
    "\n",
    "Un modèle linéaire de classification binaire sépare l'espace en deux sous-espaces par un hyperplan.\n",
    "En classification binaire, on fait souvent référence aux deux classes par les termes *classe positive* et *classe négative*.\n",
    "\n",
    "Le modèle prédit comme :\n",
    "\n",
    "* observation positive tout point du plan dont la distance signée à l'hyperplan est positive, et\n",
    "* observation négative tout point du plan dont la distance signée à l'hyperplan est négative.\n",
    "\n",
    "\n",
    "### Cas où les classes sont linéairement séparables\n",
    "\n",
    "Tout d'abord, on utilise la fonction [`sklearn.datasets.make_classification()`](https://scikit-learn.org/1.6/modules/generated/sklearn.datasets.make_classification.html) pour créer un jeu de données avec seulement deux variables en entrée pour pouvoir facilement visualiser le jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_log_sep, y_log_sep = make_classification(\n",
    "    n_samples=40, n_features=2, n_informative=2, n_repeated=0, n_redundant=0,\n",
    "    n_clusters_per_class=1, flip_y=0, class_sep=2, random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9184d0",
   "metadata": {},
   "source": [
    "Deux variables Python ont été créées :\n",
    "* `X_log_sep` correspond à la matrice $\\mathbf{X}$ contenant les entrées,\n",
    "* `y_log_sep` correspond au vecteur $\\mathbf{y}$ contenant les sorties (c'est-à-dire les classes, $0$ ou $1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf638cd",
   "metadata": {},
   "source": [
    "**Question 9 : Effectuez une visualisation du jeu de données. Est-ce que les classes sont linéairement séparables ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e0124d",
   "metadata": {},
   "source": [
    "Pour un hyperplan caractérisé par le vecteur $\\mathbf{w}$, la distance signée d'un point $\\mathbf{x}$ à l'hyperplan est le produit scalaire entre les deux vecteurs :\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}\n",
    "$$\n",
    "\n",
    "On fait encore l'hypothèse d'avoir rajouter une variable supplémentaire au vecteur $\\mathbf{x}$ contenant un `1` pour modéliser la constante.\n",
    "\n",
    "**Question 10 : Ajoutez une colonne de 1 en tant que dernière colonne à la matrice `X_log_sep` pour modéliser la constante.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21097f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047d6ed",
   "metadata": {},
   "source": [
    "La régression logistique est un modèle probabiliste avec l'hypothsèse suivante :\n",
    "\n",
    "$$\n",
    "    P(y=1 \\vert \\mathbf{x}) = \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x} \\right)\n",
    "$$\n",
    "\n",
    "où $\\sigma$ est la fonction sigmoïde définie par :\n",
    "\n",
    "$$\n",
    "    \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "**Question 11 : Définissez une fonction `sigmoid()` qui prend en argument un tableau NumPy et qui renvoie le tableau NumPy de même forme où la fonction sigmoïde a été appliquée à chaque élément du tableau.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af66c80",
   "metadata": {},
   "source": [
    "Pour trouver les paramètres optimaux (l'hyperplan $\\mathbf{w}$), on cherche encore à minimiser une fonction de coût notée $J$ :\n",
    "\n",
    "$$\n",
    "    \\mathbf{w}^* = \\arg\\min_{\\mathbf{w}} J(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "On utilise l'[entropie croisée](https://fr.wikipedia.org/wiki/Entropie_croisée) comme critère :\n",
    "\n",
    "$$\n",
    "    J(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n - y^{(i)} \\log \\left( \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} \\right) \\right)\n",
    "$$\n",
    "\n",
    "Le gradient de cette fonction par rapport à $\\mathbf{w}$ est noté $\\nabla_{\\mathbf{w}} J$ :\n",
    "\n",
    "$$\n",
    "    \\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} \\right) - y^{(i)} \\right) \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "Il n'existe pas d'[expression de forme fermée](https://fr.wikipedia.org/wiki/Expression_de_forme_fermée) pour résoudre l'équation $\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = 0$.\n",
    "On va donc implémenter à la place une [descente du gradient](https://fr.wikipedia.org/wiki/Algorithme_du_gradient) pour trouver les paramètres optimaux.\n",
    "\n",
    "**Algorithme de descente du gradient**\n",
    "\n",
    "* Paramètres :\n",
    "    + `X` : matrice des entrées\n",
    "    + `y` : vecteur des sorties\n",
    "    + `max_iter` : nombre maximum d'itérations\n",
    "    + `tol` : tolerance (sur la norme infinie du gradient)\n",
    "    + `lr` : taux d'apprentissage $\\eta$\n",
    "* Initialiser les coefficients à 0 : $\\mathbf{w}^{(0)}$\n",
    "* Calculer le gradient $\\nabla_{\\mathbf{w}} J(\\mathbf{w}^{(0)})$\n",
    "* Si la norme infinie du gradient est inférieure à `tol`\n",
    "    + Renvoyer les résultats : l'initialisation convient\n",
    "* Sinon, tant que le nombre maximum d'itérations n'est pas atteint\n",
    "    + Mettre à jour les coefficients avec la formule suivante : $\\mathbf{w}^{(t)} = \\mathbf{w}^{(t-1)} - \\eta \\times \\nabla_{\\mathbf{w}} J(\\mathbf{w}^{(t-1)})$\n",
    "    + Calculer le gradient $\\nabla_{\\mathbf{w}} J(\\mathbf{w}^{(t)})$\n",
    "    + Si la norme infinie du gradient est inférieure à `tol`\n",
    "        - Arrêter\n",
    "* Renvoyer les coefficients finaux, si l'algorithme a convergé et le nombre d'itérations effectuées.\n",
    "\n",
    "Pour rappel, la norme infinie d'un vecteur est définie par :\n",
    "$$\n",
    "    \\Vert \\mathbf{x} \\Vert_{\\infty} = \\max_{j} \\vert x_j \\vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca4302",
   "metadata": {},
   "source": [
    "**Question 12 : Définissez une fonction `norm()` qui renvoie la norme infinie d'un tableau NumPy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d78266",
   "metadata": {},
   "source": [
    "**Question 13 : Définissez une fonction `gradient_logistic_regression()` qui prend en arguments le vecteur des coefficients $\\mathbf{w}$, la matrice des entrées $\\mathbf{X}$ et le vecteur des sorties $\\mathbf{y}$, et qui renvoie le gradient $\\nabla_{\\mathbf{w}} J(\\mathbf{w})$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_logistic_regression(w, X, y):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5f4db",
   "metadata": {},
   "source": [
    "Vous vous demandez sûrement si votre implémentation du gradient est correcte. Pour la vérifier, on peut utiliser la fonction [`scipy.optimize.check_grad()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.check_grad.html) qui vérfie l'exactitude d'une fonction de gradient en la comparant à une approximation par différences finies (vers l'avant) du gradient.\n",
    "Pour utiliser cette fonction, il est nécessaire de définir la fonction originale pour laquelle on calcule le gradient.\n",
    "\n",
    "**Question 14 : Définissez une fonction `function_logistic_regression()` qui prend en arguments le vecteur des coefficients $\\mathbf{w}$, la matrice des entrées $\\mathbf{X}$ et le vecteur des sorties $\\mathbf{y}$, et qui renvoie $J(\\mathbf{w})$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f4db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_logistic_regression(w, X, y):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be6be6",
   "metadata": {},
   "source": [
    "**Question 15 : Utilisez la fonction [`scipy.optimize.check_grad()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.check_grad.html) pour vérifier votre fonction du gradient.** Vous pouvez vérifier que le résultat renvoyé par cette fonction, correspondant à la norme $L_2$ de la différence entre le gradient calculé par la fonction fournie et l'approximation calculée par différences finies, est inférieur à une faible valeur (par exemple `1e-4`). Effectuez cette vérification pour différentes valeurs de $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19b5c9",
   "metadata": {},
   "source": [
    "**Question 16 : Définissez une fonction `gradient_descent()` qui implémente l'algorithme de descente de gradient défini ci-dessus et qui renvoie un dictionnaire avec les informations suivantes :**\n",
    "\n",
    "* La clé `'coef'` a pour valeur les coefficients obtenus à la dernière itération effectuée.\n",
    "* La clé `'convergé'` a pour valeur un booléen indiquant si l'algorithme a convergé ou non.\n",
    "* La clé `'n_iter'` a pour valeur le nombre d'itérations effectuées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6384d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79196702",
   "metadata": {},
   "source": [
    "**Question 17 : Appelez la fonction `gradient_descent()` avec les valeurs suivantes : `max_iter=10_000, tol=1e-4` et le taux d'apprentissage `lr` prenant ses valeurs dans `[10.0 ** k for k in range(-3, 3)]`. Sauvegardez les résultats dans un seul dictionnaire dont les clés sont les valeurs du taux d'apprentissage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, max_iter, tol, lr):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18e60a",
   "metadata": {},
   "source": [
    "**Question 18 : Définissez une fonction `plot_decision_functions()` qui affiche sur un même graphique 6 figures, où chaque figure affiche le jeu de données et l'hyperplan appris de la régression logistique pour ce taux d'apprentissage. Indiquez dans le titre de chaque sous-figure les informations pertinentes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_functions(X, y, res):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2563f17",
   "metadata": {},
   "source": [
    "**Question 19 : Appelez cette fonction. Que constatez-vous ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc58ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ba533",
   "metadata": {},
   "source": [
    "**Question 20 : Définissez une fonction `accuracy()` qui prend en arguments le vecteur des coefficients $\\mathbf{w}$, la matrice des entrées $\\mathbf{X}$ et le vecteur des sorties $\\mathbf{y}$, et qui renvoie la proportion de bonnes prédictions effectuées par le modèle.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w, X, y):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09abe32",
   "metadata": {},
   "source": [
    "**Question 21 : Utilisez cette fonction pour vérifier que la proportion de bonnes prédictions effectuées par chacun des modèles est bien égale à 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced830bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70046848",
   "metadata": {},
   "source": [
    "### Cas où les classes ne sont pas linéairement séparables\n",
    "\n",
    "On s'intéresse maintenant au cas plus réaliste où les classes ne sont pas linéairement séparables.\n",
    "On utilise à nouveau la fonction [`sklearn.datasets.make_classification()`](https://scikit-learn.org/1.6/modules/generated/sklearn.datasets.make_classification.html) pour créer un tel jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log, y_log = make_classification(\n",
    "    n_samples=40, n_features=2, n_informative=2, n_repeated=0, n_redundant=0,\n",
    "    n_clusters_per_class=1, flip_y=0.2, class_sep=0.8, random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f7d22",
   "metadata": {},
   "source": [
    "**Question 22 : Effectuez une visualisation du jeu de données. Est-ce que les classes ne sont pas linéairement séparables ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006136b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d67da",
   "metadata": {},
   "source": [
    "**Question 23 : Ajoutez une colonne de 1 en tant que dernière colonne à la matrice `X_log` pour modéliser la constante.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e455bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd617506",
   "metadata": {},
   "source": [
    "**Question 24 : Répétez les questions 14 et 16 pour ce jeu de données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf82481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526e2e7",
   "metadata": {},
   "source": [
    "\n",
    "On constate que le taux d'apprentissage est un hyperparamètre important car :\n",
    "\n",
    "* s'il est trop élevé, l'algorithme ne vas pas converger (vers la valeur optimale), et\n",
    "* s'il est trop faible, l'algorithme va mettre trop de temps à converger.\n",
    "\n",
    "Il existe une borne supérieure qui permet d'affirmer que si le taux d'apprentissage est inférieur à cette borne supérieure, alors l'algorithme converge forcément (avec assez d'itérations) vers la solution optimale.\n",
    "Cette borne supérieure est l'inverse de la plus grande valeur propre de la matrice hessienne de la fonction de coût.\n",
    "\n",
    "La matrice hessienne de la fonction de coût est :\n",
    "$$\n",
    "    \\nabla_{\\mathbf{w}}^2 J(\\mathbf{w}) \n",
    "    = \\frac{1}{n} \\sum_{i=1}^n \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} \\right) \\left( 1 - \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} \\right) \\right) \\mathbf{x}^{(i)} \\mathbf{x}^{(i)\\top}\n",
    "    = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{D} \\mathbf{X}\n",
    "    \\qquad\\text{avec}\\qquad\n",
    "    \\mathbf{D} = \\text{diag}\\left( \\left[ \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} \\right) \\left( 1 - \\sigma \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} \\right) \\right) \\right]_{i=1}^p \\right)\n",
    "$$\n",
    "\n",
    "La matrice hessienne dépend du point d'évaluation $\\mathbf{w}$ à travers la matrice diagonale $\\mathbf{D}$. Néanmoins, si on trouve un majorant de $\\mathbf{D}$ (en valeurs propres), alors on pourra calculer un majorant de la hessienne.\n",
    "\n",
    "**Question 25 : Affichez la courbe de la fonction $x \\mapsto \\sigma(x) (1 - \\sigma(x))$ sur l'intervalle $[-10, 10]$ pour déterminer visuellement le maximum de cette fonction. En déduire une matrice majorant (en valeurs propres) la hessienne.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3677d95",
   "metadata": {},
   "source": [
    "On utilise la fonction [`scipy.sparse.linalg.svds()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html) pour calculer la plus grande valeur propre de la matrice $\\frac{1}{4n} X^\\top X$ sans avoir à calculer le produit entre les deux matrices grâce à la [décomposition en valeurs singulières](https://fr.wikipedia.org/wiki/Décomposition_en_valeurs_singulières) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136092f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "svds(X_log, k=1, which='LM', rng=42, return_singular_vectors=False).item() ** 2 / (4 * X_log.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae4e3b",
   "metadata": {},
   "source": [
    "**Question 26 : Définissez une nouvelle version de la fonction `gradient_descent_auto_lr()` qui ne prend plus un argument `lr` pour le taux d'apprentissage, mais qui le calcule automatiquement avec la méthodologie définie ci-dessus. Ajoutez le taux d'apprentissage utilisé dans le dictionnaire renvoyé par cette fonction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd015e0",
   "metadata": {},
   "source": [
    "**Question 27 : Affichez les résultats obtenus avec cette fonction dans un graphique. Comparez ces résultats avec ceux obtenus à la question 21.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e93002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
